#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot Avanc√© avec LangChain et HuggingFace
==========================================

Chatbot intelligent pour recommandations Airbnb utilisant :
- LangChain pour la gestion des conversations
- HuggingFace Transformers pour les mod√®les de langage
- FAISS pour la recherche vectorielle
- RAG (Retrieval-Augmented Generation)

Auteur: NakibIheb20
Date: 2025
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Ajouter le chemin du projet
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    # LangChain imports (versions mises √† jour)
    try:
        from langchain_community.llms import HuggingFacePipeline
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain_community.document_loaders import DataFrameLoader
    except ImportError:
        # Fallback vers les anciens imports
        from langchain.llms import HuggingFacePipeline
        from langchain.embeddings import HuggingFaceEmbeddings
        from langchain.vectorstores import FAISS
        from langchain.document_loaders import DataFrameLoader
    
    from langchain.memory import ConversationBufferMemory
    from langchain.chains import ConversationalRetrievalChain
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.prompts import PromptTemplate
    
    # HuggingFace imports
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM,
        pipeline
    )
    import torch
    
    # BitsAndBytesConfig optionnel (pour quantification)
    try:
        from transformers import BitsAndBytesConfig
    except ImportError:
        BitsAndBytesConfig = None
    
    # Interface utilisateur
    import gradio as gr
    
    LANGCHAIN_AVAILABLE = True
    print("‚úÖ LangChain et HuggingFace disponibles")
    
except ImportError as e:
    print(f"‚ö†Ô∏è Certaines d√©pendances manquent : {e}")
    print("üì¶ Installez avec : pip install langchain transformers torch gradio")
    LANGCHAIN_AVAILABLE = False


class AirbnbChatbotConfig:
    """Configuration pour le chatbot Airbnb avec LangChain"""
    
    # Mod√®les HuggingFace recommand√©s
    MODELS = {
        'small': 'microsoft/DialoGPT-small',  # Rapide, l√©ger
        'medium': 'microsoft/DialoGPT-medium',  # √âquilibr√©
        'french': 'dbmdz/bert-base-french-europeana-cased',  # Fran√ßais
        'multilingual': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
        'gpt2': 'gpt2'  # Fallback simple
    }
    
    # Configuration par d√©faut
    DEFAULT_MODEL = 'small'
    MAX_LENGTH = 512
    TEMPERATURE = 0.7
    TOP_P = 0.9
    
    # Embeddings pour la recherche s√©mantique
    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    
    # Prompts en fran√ßais
    SYSTEM_PROMPT = """
    Tu es un assistant intelligent sp√©cialis√© dans les recommandations d'h√©bergements Airbnb 
    pour la Tunisie, particuli√®rement Hammamet et Jerba.
    
    Tes responsabilit√©s :
    - Recommander des h√©bergements bas√©s sur les pr√©f√©rences utilisateur
    - Analyser les avis clients pour donner des conseils
    - Fournir des informations sur les destinations
    - √ätre amical, professionnel et informatif
    
    R√©ponds toujours en fran√ßais et sois concis mais complet.
    """


class AirbnbLangChainChatbot:
    """
    Chatbot Airbnb utilisant LangChain et HuggingFace
    """
    
    def __init__(self, model_size: str = 'small'):
        """
        Initialise le chatbot
        
        Args:
            model_size (str): Taille du mod√®le √† utiliser
        """
        self.config = AirbnbChatbotConfig()
        self.conversation_history = []
        self.model_size = model_size
        
        if LANGCHAIN_AVAILABLE:
            self._initialize_components()
        else:
            print("‚ùå LangChain non disponible - Mode d√©grad√©")
            self.conversation_chain = None
    
    def _initialize_components(self):
        """Initialise tous les composants du chatbot"""
        print("üîÑ Initialisation du chatbot LangChain...")
        
        try:
            # 1. Initialiser le mod√®le LLM
            self.llm = self._initialize_huggingface_model()
            
            # 2. Cr√©er la base de connaissances
            self.vectorstore = self._create_knowledge_base()
            
            # 3. Configurer la cha√Æne conversationnelle
            self.conversation_chain = self._create_conversation_chain()
            
            print("‚úÖ Chatbot LangChain initialis√© avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation : {e}")
            self.conversation_chain = None
    
    def _initialize_huggingface_model(self) -> Optional[HuggingFacePipeline]:
        """
        Initialise le mod√®le HuggingFace
        
        Returns:
            HuggingFacePipeline: Pipeline LangChain configur√©
        """
        print(f"üîÑ Chargement du mod√®le {self.model_size}...")
        
        model_name = self.config.MODELS.get(self.model_size, self.config.MODELS['gpt2'])
        
        try:
            # Configuration pour optimiser la m√©moire
            device = 0 if torch.cuda.is_available() else -1
            
            # Cr√©er le pipeline de g√©n√©ration de texte
            text_generation_pipeline = pipeline(
                "text-generation",
                model=model_name,
                tokenizer=model_name,
                max_length=self.config.MAX_LENGTH,
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P,
                device=device,
                do_sample=True,
                pad_token_id=50256  # Pour √©viter les warnings
            )
            
            # Wrapper LangChain
            llm = HuggingFacePipeline(
                pipeline=text_generation_pipeline,
                model_kwargs={
                    "temperature": self.config.TEMPERATURE,
                    "max_length": self.config.MAX_LENGTH
                }
            )
            
            print(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s !")
            return llm
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
            print("üîÑ Tentative avec GPT-2...")
            
            # Fallback vers GPT-2
            try:
                simple_pipeline = pipeline(
                    "text-generation",
                    model="gpt2",
                    max_length=256,
                    device=device
                )
                
                return HuggingFacePipeline(pipeline=simple_pipeline)
            except Exception as e2:
                print(f"‚ùå Impossible de charger un mod√®le : {e2}")
                return None
    
    def _create_knowledge_base(self) -> Optional[FAISS]:
        """
        Cr√©e une base de connaissances vectorielle pour le RAG
        
        Returns:
            FAISS: Base de donn√©es vectorielle
        """
        print("üìö Cr√©ation de la base de connaissances...")
        
        try:
            # Donn√©es d'exemple sur les h√©bergements Airbnb Tunisie
            knowledge_data = [
                {
                    "content": "Hammamet est une destination baln√©aire populaire en Tunisie, connue pour ses plages de sable fin et sa m√©dina historique. Les h√©bergements Airbnb y offrent souvent des vues sur mer et un acc√®s facile aux attractions touristiques.",
                    "location": "Hammamet",
                    "type": "destination_info"
                },
                {
                    "content": "Jerba est une √Æle tunisienne r√©put√©e pour son climat doux, ses plages magnifiques et son patrimoine culturel riche. Les h√©bergements traditionnels avec architecture locale sont tr√®s appr√©ci√©s des visiteurs.",
                    "location": "Jerba", 
                    "type": "destination_info"
                },
                {
                    "content": "Pour un s√©jour familial, privil√©giez les h√©bergements avec piscine, cuisine √©quip√©e et proximit√© des plages. Les villas avec jardin sont id√©ales pour les groupes.",
                    "location": "G√©n√©ral",
                    "type": "recommendation"
                },
                {
                    "content": "Les couples appr√©cient les riads traditionnels avec terrasse priv√©e, les appartements avec vue sur mer et les h√©bergements dans les m√©dinas pour une exp√©rience authentique.",
                    "location": "G√©n√©ral",
                    "type": "recommendation"
                },
                {
                    "content": "La meilleure p√©riode pour visiter la Tunisie est d'avril √† juin et de septembre √† novembre. Les prix des h√©bergements sont plus avantageux hors saison estivale.",
                    "location": "G√©n√©ral",
                    "type": "travel_tips"
                }
            ]
            
            # Cr√©er un DataFrame
            df = pd.DataFrame(knowledge_data)
            
            # Charger avec LangChain
            loader = DataFrameLoader(df, page_content_column="content")
            documents = loader.load()
            
            # Diviser les documents
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            texts = text_splitter.split_documents(documents)
            
            # Cr√©er les embeddings
            embeddings = HuggingFaceEmbeddings(
                model_name=self.config.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'}  # Utiliser CPU pour la compatibilit√©
            )
            
            # Cr√©er la base vectorielle FAISS
            vectorstore = FAISS.from_documents(texts, embeddings)
            
            print(f"‚úÖ Base de connaissances cr√©√©e avec {len(texts)} documents")
            return vectorstore
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation de la base de connaissances : {e}")
            return None
    
    def _create_conversation_chain(self) -> Optional[ConversationalRetrievalChain]:
        """
        Cr√©e une cha√Æne conversationnelle avec m√©moire et RAG
        
        Returns:
            ConversationalRetrievalChain: Cha√Æne conversationnelle
        """
        if not self.llm or not self.vectorstore:
            print("‚ùå Impossible de cr√©er la cha√Æne conversationnelle")
            return None
        
        print("üîó Configuration de la cha√Æne conversationnelle...")
        
        try:
            # M√©moire conversationnelle
            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            
            # Cr√©er la cha√Æne conversationnelle
            qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vectorstore.as_retriever(
                    search_kwargs={"k": 3}  # R√©cup√©rer les 3 documents les plus pertinents
                ),
                memory=memory,
                return_source_documents=True,
                verbose=True
            )
            
            print("‚úÖ Cha√Æne conversationnelle configur√©e")
            return qa_chain
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation de la cha√Æne : {e}")
            return None
    
    def chat(self, user_input: str) -> Dict[str, Any]:
        """
        Traite une question utilisateur et retourne une r√©ponse
        
        Args:
            user_input (str): Question de l'utilisateur
        
        Returns:
            Dict: R√©ponse avec m√©tadonn√©es
        """
        if not self.conversation_chain:
            return self._fallback_response(user_input)
        
        try:
            # Obtenir la r√©ponse de la cha√Æne conversationnelle
            result = self.conversation_chain({
                "question": user_input
            })
            
            # Extraire la r√©ponse
            answer = result.get("answer", "D√©sol√©, je n'ai pas pu traiter votre demande.")
            source_docs = result.get("source_documents", [])
            
            # Ajouter √† l'historique
            self.conversation_history.append({
                "user": user_input,
                "bot": answer,
                "sources": len(source_docs)
            })
            
            return {
                "answer": answer,
                "sources": source_docs,
                "confidence": self._calculate_confidence(answer, source_docs),
                "method": "langchain"
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement : {e}")
            return self._fallback_response(user_input)
    
    def _fallback_response(self, user_input: str) -> Dict[str, Any]:
        """
        R√©ponse de secours quand LangChain n'est pas disponible
        
        Args:
            user_input (str): Question de l'utilisateur
        
        Returns:
            Dict: R√©ponse de base
        """
        # R√©ponses pr√©d√©finies simples
        responses = {
            "hammamet": "Hammamet est une excellente destination avec de belles plages et une m√©dina historique. Je recommande les h√©bergements avec vue sur mer.",
            "jerba": "Jerba est parfaite pour des vacances relaxantes avec son climat doux et ses plages magnifiques. Les h√©bergements traditionnels sont tr√®s appr√©ci√©s.",
            "famille": "Pour les familles, je conseille les villas avec piscine et cuisine √©quip√©e, proche des plages et des activit√©s pour enfants.",
            "couple": "Les couples appr√©cient les riads avec terrasse priv√©e et les appartements romantiques avec vue sur mer.",
            "p√©riode": "La meilleure p√©riode est d'avril √† juin et de septembre √† novembre pour √©viter la forte chaleur estivale."
        }
        
        user_lower = user_input.lower()
        
        # Recherche de mots-cl√©s
        for keyword, response in responses.items():
            if keyword in user_lower:
                return {
                    "answer": response,
                    "sources": [],
                    "confidence": 0.6,
                    "method": "fallback"
                }
        
        # R√©ponse par d√©faut
        return {
            "answer": "Bonjour ! Je suis votre assistant pour les h√©bergements Airbnb en Tunisie. Posez-moi vos questions sur Hammamet, Jerba, ou vos pr√©f√©rences de voyage !",
            "sources": [],
            "confidence": 0.5,
            "method": "fallback"
        }
    
    def _calculate_confidence(self, answer: str, sources: List) -> float:
        """
        Calcule un score de confiance bas√© sur la r√©ponse et les sources
        
        Args:
            answer (str): R√©ponse g√©n√©r√©e
            sources (List): Sources utilis√©es
        
        Returns:
            float: Score de confiance (0-1)
        """
        if not answer or "d√©sol√©" in answer.lower():
            return 0.2
        
        # Plus il y a de sources, plus la confiance est √©lev√©e
        source_score = min(len(sources) * 0.3, 0.9)
        
        # Longueur de la r√©ponse (r√©ponses plus d√©taill√©es = plus de confiance)
        length_score = min(len(answer) / 200, 0.5)
        
        return min(source_score + length_score + 0.3, 1.0)
    
    def get_conversation_history(self) -> List[Dict]:
        """Retourne l'historique de conversation"""
        return self.conversation_history
    
    def clear_history(self):
        """Efface l'historique de conversation"""
        self.conversation_history = []
        if self.conversation_chain and hasattr(self.conversation_chain, 'memory'):
            self.conversation_chain.memory.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Retourne les statistiques du chatbot
        
        Returns:
            Dict: Statistiques d'utilisation
        """
        history = self.get_conversation_history()
        
        if not history:
            return {
                "total_conversations": 0,
                "avg_confidence": 0.0,
                "total_sources_used": 0,
                "langchain_enabled": LANGCHAIN_AVAILABLE
            }
        
        confidences = [conv.get("confidence", 0) for conv in history if "confidence" in conv]
        sources = [conv.get("sources", 0) for conv in history if "sources" in conv]
        
        return {
            "total_conversations": len(history),
            "avg_confidence": np.mean(confidences) if confidences else 0.0,
            "total_sources_used": sum(sources),
            "langchain_enabled": LANGCHAIN_AVAILABLE and self.conversation_chain is not None
        }


def create_gradio_interface(chatbot: AirbnbLangChainChatbot):
    """
    Cr√©e une interface Gradio pour le chatbot
    
    Args:
        chatbot: Instance du chatbot
    
    Returns:
        gr.Blocks: Interface Gradio
    """
    def chat_interface(message, history):
        """Interface de chat pour Gradio"""
        if not message.strip():
            return history, ""
        
        # Obtenir la r√©ponse du chatbot
        response = chatbot.chat(message)
        
        # Ajouter √† l'historique Gradio
        history.append([message, response['answer']])
        
        return history, ""
    
    def clear_chat():
        """Efface l'historique de chat"""
        chatbot.clear_history()
        return [], ""
    
    def show_stats():
        """Affiche les statistiques"""
        stats = chatbot.get_stats()
        return f"""
        üìä **Statistiques du Chatbot**
        - Conversations: {stats['total_conversations']}
        - Confiance moyenne: {stats['avg_confidence']:.2f}
        - Sources utilis√©es: {stats['total_sources_used']}
        - LangChain actif: {'‚úÖ' if stats['langchain_enabled'] else '‚ùå'}
        """
    
    # Cr√©er l'interface Gradio
    with gr.Blocks(
        title="üè† Chatbot Airbnb Tunisie - LangChain + HuggingFace",
        theme=gr.themes.Soft()
    ) as interface:
        
        gr.Markdown("""
        # ü§ñ Assistant Airbnb Intelligent
        
        Powered by **LangChain** + **HuggingFace** üöÄ
        
        Posez-moi vos questions sur les h√©bergements Airbnb en Tunisie !
        """)
        
        with gr.Tab("üí¨ Chat"):
            chatbot_interface = gr.Chatbot(
                label="Conversation",
                height=400,
                show_label=True
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="Votre message",
                    placeholder="Tapez votre question ici...",
                    scale=4
                )
                send_btn = gr.Button("Envoyer üì§", scale=1)
            
            with gr.Row():
                clear_btn = gr.Button("Effacer üóëÔ∏è")
                
            # Exemples de questions
            gr.Examples(
                examples=[
                    "Recommande-moi un h√©bergement √† Hammamet",
                    "Quels sont les avantages de Jerba ?",
                    "Je voyage en famille, que conseilles-tu ?",
                    "Quelle est la meilleure p√©riode pour visiter ?"
                ],
                inputs=msg
            )
        
        with gr.Tab("üìä Statistiques"):
            stats_btn = gr.Button("Actualiser les statistiques")
            stats_output = gr.Markdown()
        
        # √âv√©nements
        send_btn.click(
            chat_interface,
            inputs=[msg, chatbot_interface],
            outputs=[chatbot_interface, msg]
        )
        
        msg.submit(
            chat_interface,
            inputs=[msg, chatbot_interface],
            outputs=[chatbot_interface, msg]
        )
        
        clear_btn.click(
            clear_chat,
            outputs=[chatbot_interface, msg]
        )
        
        stats_btn.click(
            show_stats,
            outputs=stats_output
        )
    
    return interface


def main():
    """Fonction principale"""
    print("üöÄ D√©marrage du Chatbot Airbnb LangChain...")
    print("=" * 50)
    
    # Initialiser le chatbot
    chatbot = AirbnbLangChainChatbot(model_size='small')
    
    # Tests rapides
    print("\nüß™ Tests du chatbot...")
    test_questions = [
        "Bonjour !",
        "Recommande-moi un h√©bergement √† Hammamet",
        "Je voyage en famille"
    ]
    
    for question in test_questions:
        print(f"\nüôã {question}")
        response = chatbot.chat(question)
        print(f"ü§ñ {response['answer'][:100]}...")
        print(f"üìä Confiance: {response['confidence']:.2f} | M√©thode: {response['method']}")
    
    # Afficher les statistiques
    stats = chatbot.get_stats()
    print(f"\nüìä Statistiques finales:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # Cr√©er l'interface Gradio
    if LANGCHAIN_AVAILABLE:
        print("\nüé® Cr√©ation de l'interface Gradio...")
        interface = create_gradio_interface(chatbot)
        
        print("‚úÖ Interface pr√™te !")
        print("üí° D√©commentez la ligne suivante pour lancer l'interface web:")
        print("# interface.launch(share=True, debug=True)")
        
        # D√©commentez pour lancer l'interface
        # interface.launch(share=True, debug=True)
    else:
        print("‚ö†Ô∏è Interface Gradio non disponible sans LangChain")
    
    return chatbot


if __name__ == "__main__":
    chatbot = main()