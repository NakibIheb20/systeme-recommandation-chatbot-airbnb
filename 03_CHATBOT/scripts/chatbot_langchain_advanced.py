#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot AvancÃ© avec LangChain et HuggingFace
==========================================

Chatbot intelligent pour recommandations Airbnb utilisant :
- LangChain pour la gestion des conversations
- HuggingFace Transformers pour les modÃ¨les de langage
- FAISS pour la recherche vectorielle
- RAG (Retrieval-Augmented Generation)

Auteur: NakibIheb20
Date: 2025
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Ajouter le chemin du projet
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    # LangChain imports (versions mises Ã  jour)
    try:
        from langchain_community.llms import HuggingFacePipeline
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain_community.document_loaders import DataFrameLoader
    except ImportError:
        # Fallback vers les anciens imports
        from langchain.llms import HuggingFacePipeline
        from langchain.embeddings import HuggingFaceEmbeddings
        from langchain.vectorstores import FAISS
        from langchain.document_loaders import DataFrameLoader
    
    from langchain.memory import ConversationBufferMemory
    from langchain.chains import ConversationalRetrievalChain
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.prompts import PromptTemplate
    
    # HuggingFace imports
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM,
        pipeline
    )
    import torch
    
    # BitsAndBytesConfig optionnel (pour quantification)
    try:
        from transformers import BitsAndBytesConfig
    except ImportError:
        BitsAndBytesConfig = None
    
    # Interface utilisateur
    import gradio as gr
    
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain et HuggingFace disponibles")
    
except ImportError as e:
    print(f"âš ï¸ Certaines dÃ©pendances manquent : {e}")
    print("ğŸ“¦ Installez avec : pip install langchain transformers torch gradio")
    LANGCHAIN_AVAILABLE = False


class AirbnbChatbotConfig:
    """Configuration pour le chatbot Airbnb avec LangChain"""
    
    # ModÃ¨les HuggingFace recommandÃ©s
    MODELS = {
        'small': 'microsoft/DialoGPT-small',  # Rapide, lÃ©ger
        'medium': 'microsoft/DialoGPT-medium',  # Ã‰quilibrÃ©
        'french': 'dbmdz/bert-base-french-europeana-cased',  # FranÃ§ais
        'multilingual': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
        'gpt2': 'gpt2'  # Fallback simple
    }
    
    # Configuration par dÃ©faut
    DEFAULT_MODEL = 'small'
    MAX_LENGTH = 512
    TEMPERATURE = 0.7
    TOP_P = 0.9
    
    # Embeddings pour la recherche sÃ©mantique
    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    
    # Prompts en franÃ§ais
    SYSTEM_PROMPT = """
    Tu es un assistant intelligent spÃ©cialisÃ© dans les recommandations d'hÃ©bergements Airbnb 
    pour la Tunisie, particuliÃ¨rement Hammamet et Jerba.
    
    Tes responsabilitÃ©s :
    - Recommander des hÃ©bergements basÃ©s sur les prÃ©fÃ©rences utilisateur
    - Analyser les avis clients pour donner des conseils
    - Fournir des informations sur les destinations
    - ÃŠtre amical, professionnel et informatif
    
    RÃ©ponds toujours en franÃ§ais et sois concis mais complet.
    """


class AirbnbLangChainChatbot:
    """
    Chatbot Airbnb utilisant LangChain et HuggingFace
    """
    
    def __init__(self, model_size: str = 'small'):
        """
        Initialise le chatbot
        
        Args:
            model_size (str): Taille du modÃ¨le Ã  utiliser
        """
        self.config = AirbnbChatbotConfig()
        self.conversation_history = []
        self.model_size = model_size
        
        if LANGCHAIN_AVAILABLE:
            self._initialize_components()
        else:
            print("âŒ LangChain non disponible - Mode dÃ©gradÃ©")
            self.conversation_chain = None
    
    def _initialize_components(self):
        """Initialise tous les composants du chatbot"""
        print("ğŸ”„ Initialisation du chatbot LangChain...")
        
        try:
            # 1. Initialiser le modÃ¨le LLM
            self.llm = self._initialize_huggingface_model()
            
            # 2. CrÃ©er la base de connaissances
            self.vectorstore = self._create_knowledge_base()
            
            # 3. Configurer la chaÃ®ne conversationnelle
            self.conversation_chain = self._create_conversation_chain()
            
            print("âœ… Chatbot LangChain initialisÃ© avec succÃ¨s !")
            
        except Exception as e:
            print(f"âŒ Erreur lors de l'initialisation : {e}")
            self.conversation_chain = None
    
    def _initialize_huggingface_model(self) -> Optional[HuggingFacePipeline]:
        """
        Initialise le modÃ¨le HuggingFace
        
        Returns:
            HuggingFacePipeline: Pipeline LangChain configurÃ©
        """
        print(f"ğŸ”„ Chargement du modÃ¨le {self.model_size}...")
        
        model_name = self.config.MODELS.get(self.model_size, self.config.MODELS['gpt2'])
        
        try:
            # Configuration pour optimiser la mÃ©moire
            device = 0 if torch.cuda.is_available() else -1
            
            # CrÃ©er le pipeline de gÃ©nÃ©ration de texte
            text_generation_pipeline = pipeline(
                "text-generation",
                model=model_name,
                tokenizer=model_name,
                max_length=self.config.MAX_LENGTH,
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P,
                device=device,
                do_sample=True,
                pad_token_id=50256  # Pour Ã©viter les warnings
            )
            
            # Wrapper LangChain
            llm = HuggingFacePipeline(
                pipeline=text_generation_pipeline,
                model_kwargs={
                    "temperature": self.config.TEMPERATURE,
                    "max_length": self.config.MAX_LENGTH
                }
            )
            
            print(f"âœ… ModÃ¨le {model_name} chargÃ© avec succÃ¨s !")
            return llm
            
        except Exception as e:
            print(f"âŒ Erreur lors du chargement du modÃ¨le : {e}")
            print("ğŸ”„ Tentative avec GPT-2...")
            
            # Fallback vers GPT-2
            try:
                simple_pipeline = pipeline(
                    "text-generation",
                    model="gpt2",
                    max_length=256,
                    device=device
                )
                
                return HuggingFacePipeline(pipeline=simple_pipeline)
            except Exception as e2:
                print(f"âŒ Impossible de charger un modÃ¨le : {e2}")
                return None
    
    def _create_knowledge_base(self) -> Optional[FAISS]:
        """
        CrÃ©e une base de connaissances vectorielle pour le RAG
        
        Returns:
            FAISS: Base de donnÃ©es vectorielle
        """
        print("ğŸ“š CrÃ©ation de la base de connaissances...")
        
        try:
            # DonnÃ©es d'exemple sur les hÃ©bergements Airbnb Tunisie
            knowledge_data = [
                {
                    "content": "Hammamet est une destination balnÃ©aire populaire en Tunisie, connue pour ses plages de sable fin et sa mÃ©dina historique. Les hÃ©bergements Airbnb y offrent souvent des vues sur mer et un accÃ¨s facile aux attractions touristiques.",
                    "location": "Hammamet",
                    "type": "destination_info"
                },
                {
                    "content": "Jerba est une Ã®le tunisienne rÃ©putÃ©e pour son climat doux, ses plages magnifiques et son patrimoine culturel riche. Les hÃ©bergements traditionnels avec architecture locale sont trÃ¨s apprÃ©ciÃ©s des visiteurs.",
                    "location": "Jerba", 
                    "type": "destination_info"
                },
                {
                    "content": "Pour un sÃ©jour familial, privilÃ©giez les hÃ©bergements avec piscine, cuisine Ã©quipÃ©e et proximitÃ© des plages. Les villas avec jardin sont idÃ©ales pour les groupes.",
                    "location": "GÃ©nÃ©ral",
                    "type": "recommendation"
                },
                {
                    "content": "Les couples apprÃ©cient les riads traditionnels avec terrasse privÃ©e, les appartements avec vue sur mer et les hÃ©bergements dans les mÃ©dinas pour une expÃ©rience authentique.",
                    "location": "GÃ©nÃ©ral",
                    "type": "recommendation"
                },
                {
                    "content": "La meilleure pÃ©riode pour visiter la Tunisie est d'avril Ã  juin et de septembre Ã  novembre. Les prix des hÃ©bergements sont plus avantageux hors saison estivale.",
                    "location": "GÃ©nÃ©ral",
                    "type": "travel_tips"
                }
            ]
            
            # CrÃ©er un DataFrame
            df = pd.DataFrame(knowledge_data)
            
            # Charger avec LangChain
            loader = DataFrameLoader(df, page_content_column="content")
            documents = loader.load()
            
            # Diviser les documents
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            texts = text_splitter.split_documents(documents)
            
            # CrÃ©er les embeddings
            embeddings = HuggingFaceEmbeddings(
                model_name=self.config.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'}  # Utiliser CPU pour la compatibilitÃ©
            )
            
            # CrÃ©er la base vectorielle FAISS
            vectorstore = FAISS.from_documents(texts, embeddings)
            
            print(f"âœ… Base de connaissances crÃ©Ã©e avec {len(texts)} documents")
            return vectorstore
            
        except Exception as e:
            print(f"âŒ Erreur lors de la crÃ©ation de la base de connaissances : {e}")
            return None
    
    def _create_conversation_chain(self) -> Optional[ConversationalRetrievalChain]:
        """
        CrÃ©e une chaÃ®ne conversationnelle avec mÃ©moire et RAG
        
        Returns:
            ConversationalRetrievalChain: ChaÃ®ne conversationnelle
        """
        if not self.llm or not self.vectorstore:
            print("âŒ Impossible de crÃ©er la chaÃ®ne conversationnelle")
            return None
        
        print("ğŸ”— Configuration de la chaÃ®ne conversationnelle...")
        
        try:
            # MÃ©moire conversationnelle
            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            
            # CrÃ©er la chaÃ®ne conversationnelle
            qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vectorstore.as_retriever(
                    search_kwargs={"k": 3}  # RÃ©cupÃ©rer les 3 documents les plus pertinents
                ),
                memory=memory,
                return_source_documents=True,
                verbose=True
            )
            
            print("âœ… ChaÃ®ne conversationnelle configurÃ©e")
            return qa_chain
            
        except Exception as e:
            print(f"âŒ Erreur lors de la crÃ©ation de la chaÃ®ne : {e}")
            return None
    
    def chat(self, user_input: str) -> Dict[str, Any]:
        """
        Traite une question utilisateur et retourne une rÃ©ponse
        
        Args:
            user_input (str): Question de l'utilisateur
        
        Returns:
            Dict: RÃ©ponse avec mÃ©tadonnÃ©es
        """
        if not self.conversation_chain:
            return self._fallback_response(user_input)
        
        try:
            # Obtenir la rÃ©ponse de la chaÃ®ne conversationnelle
            result = self.conversation_chain({
                "question": user_input
            })
            
            # Extraire la rÃ©ponse
            answer = result.get("answer", "DÃ©solÃ©, je n'ai pas pu traiter votre demande.")
            source_docs = result.get("source_documents", [])
            
            # Ajouter Ã  l'historique
            self.conversation_history.append({
                "user": user_input,
                "bot": answer,
                "sources": len(source_docs)
            })
            
            return {
                "answer": answer,
                "sources": source_docs,
                "confidence": self._calculate_confidence(answer, source_docs),
                "method": "langchain"
            }
            
        except Exception as e:
            print(f"âŒ Erreur lors du traitement : {e}")
            return self._fallback_response(user_input)
    
    def _fallback_response(self, user_input: str) -> Dict[str, Any]:
        """
        RÃ©ponse de secours quand LangChain n'est pas disponible
        
        Args:
            user_input (str): Question de l'utilisateur
        
        Returns:
            Dict: RÃ©ponse de base
        """
        # RÃ©ponses prÃ©dÃ©finies simples
        responses = {
            "hammamet": "Hammamet est une excellente destination avec de belles plages et une mÃ©dina historique. Je recommande les hÃ©bergements avec vue sur mer.",
            "jerba": "Jerba est parfaite pour des vacances relaxantes avec son climat doux et ses plages magnifiques. Les hÃ©bergements traditionnels sont trÃ¨s apprÃ©ciÃ©s.",
            "famille": "Pour les familles, je conseille les villas avec piscine et cuisine Ã©quipÃ©e, proche des plages et des activitÃ©s pour enfants.",
            "couple": "Les couples apprÃ©cient les riads avec terrasse privÃ©e et les appartements romantiques avec vue sur mer.",
            "pÃ©riode": "La meilleure pÃ©riode est d'avril Ã  juin et de septembre Ã  novembre pour Ã©viter la forte chaleur estivale."
        }
        
        user_lower = user_input.lower()
        
        # Recherche de mots-clÃ©s
        for keyword, response in responses.items():
            if keyword in user_lower:
                return {
                    "answer": response,
                    "sources": [],
                    "confidence": 0.6,
                    "method": "fallback"
                }
        
        # RÃ©ponse par dÃ©faut
        return {
            "answer": "Bonjour ! Je suis votre assistant pour les hÃ©bergements Airbnb en Tunisie. Posez-moi vos questions sur Hammamet, Jerba, ou vos prÃ©fÃ©rences de voyage !",
            "sources": [],
            "confidence": 0.5,
            "method": "fallback"
        }
    
    def _calculate_confidence(self, answer: str, sources: List) -> float:
        """
        Calcule un score de confiance basÃ© sur la rÃ©ponse et les sources
        
        Args:
            answer (str): RÃ©ponse gÃ©nÃ©rÃ©e
            sources (List): Sources utilisÃ©es
        
        Returns:
            float: Score de confiance (0-1)
        """
        if not answer or "dÃ©solÃ©" in answer.lower():
            return 0.2
        
        # Plus il y a de sources, plus la confiance est Ã©levÃ©e
        source_score = min(len(sources) * 0.3, 0.9)
        
        # Longueur de la rÃ©ponse (rÃ©ponses plus dÃ©taillÃ©es = plus de confiance)
        length_score = min(len(answer) / 200, 0.5)
        
        return min(source_score + length_score + 0.3, 1.0)
    
    def get_conversation_history(self) -> List[Dict]:
        """Retourne l'historique de conversation"""
        return self.conversation_history
    
    def clear_history(self):
        """Efface l'historique de conversation"""
        self.conversation_history = []
        if self.conversation_chain and hasattr(self.conversation_chain, 'memory'):
            self.conversation_chain.memory.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Retourne les statistiques du chatbot
        
        Returns:
            Dict: Statistiques d'utilisation
        """
        history = self.get_conversation_history()
        
        if not history:
            return {
                "total_conversations": 0,
                "avg_confidence": 0.0,
                "total_sources_used": 0,
                "langchain_enabled": LANGCHAIN_AVAILABLE
            }
        
        confidences = [conv.get("confidence", 0) for conv in history if "confidence" in conv]
        sources = [conv.get("sources", 0) for conv in history if "sources" in conv]
        
        return {
            "total_conversations": len(history),
            "avg_confidence": np.mean(confidences) if confidences else 0.0,
            "total_sources_used": sum(sources),
            "langchain_enabled": LANGCHAIN_AVAILABLE and self.conversation_chain is not None
        }


def create_gradio_interface(chatbot: AirbnbLangChainChatbot):
    """
    CrÃ©e une interface Gradio pour le chatbot
    
    Args:
        chatbot: Instance du chatbot
    
    Returns:
        gr.Blocks: Interface Gradio
    """
    def chat_interface(message, history):
        """Interface de chat pour Gradio"""
        if not message.strip():
            return history, ""
        
        # Obtenir la rÃ©ponse du chatbot
        response = chatbot.chat(message)
        
        # Ajouter Ã  l'historique Gradio
        history.append([message, response['answer']])
        
        return history, ""
    
    def clear_chat():
        """Efface l'historique de chat"""
        chatbot.clear_history()
        return [], ""
    
    def show_stats():
        """Affiche les statistiques"""
        stats = chatbot.get_stats()
        return f"""
        ğŸ“Š **Statistiques du Chatbot**
        - Conversations: {stats['total_conversations']}
        - Confiance moyenne: {stats['avg_confidence']:.2f}
        - Sources utilisÃ©es: {stats['total_sources_used']}
        - LangChain actif: {'âœ…' if stats['langchain_enabled'] else 'âŒ'}
        """
    
    # CrÃ©er l'interface Gradio
    with gr.Blocks(
        title="ğŸ  Chatbot Airbnb Tunisie - LangChain + HuggingFace",
        theme=gr.themes.Soft()
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¤– Assistant Airbnb Intelligent
        
        Powered by **LangChain** + **HuggingFace** ğŸš€
        
        Posez-moi vos questions sur les hÃ©bergements Airbnb en Tunisie !
        """)
        
        with gr.Tab("ğŸ’¬ Chat"):
            chatbot_interface = gr.Chatbot(
                label="Conversation",
                height=400,
                show_label=True
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="Votre message",
                    placeholder="Tapez votre question ici...",
                    scale=4
                )
                send_btn = gr.Button("Envoyer ğŸ“¤", scale=1)
            
            with gr.Row():
                clear_btn = gr.Button("Effacer ğŸ—‘ï¸")
                
            # Exemples de questions
            gr.Examples(
                examples=[
                    "Recommande-moi un hÃ©bergement Ã  Hammamet",
                    "Quels sont les avantages de Jerba ?",
                    "Je voyage en famille, que conseilles-tu ?",
                    "Quelle est la meilleure pÃ©riode pour visiter ?"
                ],
                inputs=msg
            )
        
        with gr.Tab("ğŸ“Š Statistiques"):
            stats_btn = gr.Button("Actualiser les statistiques")
            stats_output = gr.Markdown()
        
        # Ã‰vÃ©nements
        send_btn.click(
            chat_interface,
            inputs=[msg, chatbot_interface],
            outputs=[chatbot_interface, msg]
        )
        
        msg.submit(
            chat_interface,
            inputs=[msg, chatbot_interface],
            outputs=[chatbot_interface, msg]
        )
        
        clear_btn.click(
            clear_chat,
            outputs=[chatbot_interface, msg]
        )
        
        stats_btn.click(
            show_stats,
            outputs=stats_output
        )
    
    return interface


def main():
    """Fonction principale"""
    print("ğŸš€ DÃ©marrage du Chatbot Airbnb LangChain...")
    print("=" * 50)
    
    # Initialiser le chatbot
    chatbot = AirbnbLangChainChatbot(model_size='small')
    
    # Tests rapides
    print("\nğŸ§ª Tests du chatbot...")
    test_questions = [
        "Bonjour !",
        "Recommande-moi un hÃ©bergement Ã  Hammamet",
        "Je voyage en famille"
    ]
    
    for question in test_questions:
        print(f"\nğŸ™‹ {question}")
        response = chatbot.chat(question)
        print(f"ğŸ¤– {response['answer'][:100]}...")
        print(f"ğŸ“Š Confiance: {response['confidence']:.2f} | MÃ©thode: {response['method']}")
    
    # Afficher les statistiques
    stats = chatbot.get_stats()
    print(f"\nğŸ“Š Statistiques finales:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # CrÃ©er l'interface Gradio
    if LANGCHAIN_AVAILABLE:
        print("\nğŸ¨ CrÃ©ation de l'interface Gradio...")
        interface = create_gradio_interface(chatbot)
        
        print("âœ… Interface prÃªte !")
        print("ğŸ’¡ DÃ©commentez la ligne suivante pour lancer l'interface web:")
        print("# interface.launch(share=True, debug=True)")
        
        # DÃ©commentez pour lancer l'interface
        # interface.launch(share=True, debug=True)
    else:
        print("âš ï¸ Interface Gradio non disponible sans LangChain")
    
    return chatbot


if __name__ == "__main__":
    chatbot = main()