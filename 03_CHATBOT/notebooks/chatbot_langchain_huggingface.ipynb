{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Chatbot Avanc√© avec LangChain et HuggingFace\n",
    "\n",
    "## üìã **Objectifs**\n",
    "- Int√©grer **LangChain** pour la gestion des conversations\n",
    "- Utiliser **HuggingFace Transformers** pour les mod√®les de langage\n",
    "- Cr√©er un chatbot intelligent pour recommandations Airbnb\n",
    "- Impl√©menter la m√©moire conversationnelle\n",
    "- Ajouter des capacit√©s de RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## üõ†Ô∏è **Technologies Utilis√©es**\n",
    "- **LangChain** : Framework pour applications LLM\n",
    "- **HuggingFace Transformers** : Mod√®les de langage pr√©-entra√Æn√©s\n",
    "- **FAISS** : Recherche vectorielle rapide\n",
    "- **Sentence Transformers** : Embeddings s√©mantiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ **Installation des D√©pendances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages n√©cessaires\n",
    "!pip install langchain langchain-community langchain-huggingface\n",
    "!pip install transformers torch sentence-transformers\n",
    "!pip install faiss-cpu chromadb\n",
    "!pip install gradio streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö **Imports et Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# HuggingFace imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    pipeline, BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Interface utilisateur\n",
    "import gradio as gr\n",
    "\n",
    "print(\"‚úÖ Tous les imports r√©ussis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **Configuration du Mod√®le HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbChatbotConfig:\n",
    "    \"\"\"Configuration pour le chatbot Airbnb\"\"\"\n",
    "    \n",
    "    # Mod√®les HuggingFace recommand√©s\n",
    "    MODELS = {\n",
    "        'small': 'microsoft/DialoGPT-small',  # Rapide, l√©ger\n",
    "        'medium': 'microsoft/DialoGPT-medium',  # √âquilibr√©\n",
    "        'french': 'dbmdz/bert-base-french-europeana-cased',  # Fran√ßais\n",
    "        'multilingual': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    }\n",
    "    \n",
    "    # Configuration par d√©faut\n",
    "    DEFAULT_MODEL = 'small'\n",
    "    MAX_LENGTH = 512\n",
    "    TEMPERATURE = 0.7\n",
    "    TOP_P = 0.9\n",
    "    \n",
    "    # Embeddings pour la recherche s√©mantique\n",
    "    EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    \n",
    "    # Prompts en fran√ßais\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    Tu es un assistant intelligent sp√©cialis√© dans les recommandations d'h√©bergements Airbnb \n",
    "    pour la Tunisie, particuli√®rement Hammamet et Jerba.\n",
    "    \n",
    "    Tes responsabilit√©s :\n",
    "    - Recommander des h√©bergements bas√©s sur les pr√©f√©rences utilisateur\n",
    "    - Analyser les avis clients pour donner des conseils\n",
    "    - Fournir des informations sur les destinations\n",
    "    - √ätre amical, professionnel et informatif\n",
    "    \n",
    "    R√©ponds toujours en fran√ßais et sois concis mais complet.\n",
    "    \"\"\"\n",
    "\nconfig = AirbnbChatbotConfig()\nprint(\"‚öôÔ∏è Configuration initialis√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Initialisation du Mod√®le HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_huggingface_model(model_size='small'):\n",
    "    \"\"\"\n",
    "    Initialise le mod√®le HuggingFace pour la g√©n√©ration de texte\n",
    "    \n",
    "    Args:\n",
    "        model_size (str): Taille du mod√®le ('small', 'medium', 'french')\n",
    "    \n",
    "    Returns:\n",
    "        HuggingFacePipeline: Pipeline LangChain configur√©\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Chargement du mod√®le {model_size}...\")\n",
    "    \n",
    "    model_name = config.MODELS[model_size]\n",
    "    \n",
    "    try:\n",
    "        # Configuration pour optimiser la m√©moire\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        \n",
    "        # Cr√©er le pipeline de g√©n√©ration de texte\n",
    "        text_generation_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            tokenizer=model_name,\n",
    "            max_length=config.MAX_LENGTH,\n",
    "            temperature=config.TEMPERATURE,\n",
    "            top_p=config.TOP_P,\n",
    "            device=device,\n",
    "            do_sample=True,\n",
    "            pad_token_id=50256  # Pour √©viter les warnings\n",
    "        )\n",
    "        \n",
    "        # Wrapper LangChain\n",
    "        llm = HuggingFacePipeline(\n",
    "            pipeline=text_generation_pipeline,\n",
    "            model_kwargs={\n",
    "                \"temperature\": config.TEMPERATURE,\n",
    "                \"max_length\": config.MAX_LENGTH\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Mod√®le {model_name} charg√© avec succ√®s !\")\n",
    "        return llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement du mod√®le : {e}\")\n",
    "        print(\"üîÑ Tentative avec un mod√®le plus simple...\")\n",
    "        \n",
    "        # Fallback vers un mod√®le plus simple\n",
    "        simple_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"gpt2\",\n",
    "            max_length=256,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        return HuggingFacePipeline(pipeline=simple_pipeline)\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "llm = initialize_huggingface_model('small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç **Configuration des Embeddings et Base de Connaissances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_base():\n",
    "    \"\"\"\n",
    "    Cr√©e une base de connaissances vectorielle pour le RAG\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Base de donn√©es vectorielle\n",
    "    \"\"\"\n",
    "    print(\"üìö Cr√©ation de la base de connaissances...\")\n",
    "    \n",
    "    # Donn√©es d'exemple sur les h√©bergements Airbnb Tunisie\n",
    "    knowledge_data = [\n",
    "        {\n",
    "            \"content\": \"Hammamet est une destination baln√©aire populaire en Tunisie, connue pour ses plages de sable fin et sa m√©dina historique. Les h√©bergements Airbnb y offrent souvent des vues sur mer et un acc√®s facile aux attractions touristiques.\",\n",
    "            \"location\": \"Hammamet\",\n",
    "            \"type\": \"destination_info\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Jerba est une √Æle tunisienne r√©put√©e pour son climat doux, ses plages magnifiques et son patrimoine culturel riche. Les h√©bergements traditionnels avec architecture locale sont tr√®s appr√©ci√©s des visiteurs.\",\n",
    "            \"location\": \"Jerba\", \n",
    "            \"type\": \"destination_info\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Pour un s√©jour familial, privil√©giez les h√©bergements avec piscine, cuisine √©quip√©e et proximit√© des plages. Les villas avec jardin sont id√©ales pour les groupes.\",\n",
    "            \"location\": \"G√©n√©ral\",\n",
    "            \"type\": \"recommendation\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Les couples appr√©cient les riads traditionnels avec terrasse priv√©e, les appartements avec vue sur mer et les h√©bergements dans les m√©dinas pour une exp√©rience authentique.\",\n",
    "            \"location\": \"G√©n√©ral\",\n",
    "            \"type\": \"recommendation\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"La meilleure p√©riode pour visiter la Tunisie est d'avril √† juin et de septembre √† novembre. Les prix des h√©bergements sont plus avantageux hors saison estivale.\",\n",
    "            \"location\": \"G√©n√©ral\",\n",
    "            \"type\": \"travel_tips\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Cr√©er un DataFrame\n",
    "    df = pd.DataFrame(knowledge_data)\n",
    "    \n",
    "    # Charger avec LangChain\n",
    "    loader = DataFrameLoader(df, page_content_column=\"content\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Diviser les documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Cr√©er les embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=config.EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': 'cpu'}  # Utiliser CPU pour la compatibilit√©\n",
    "    )\n",
    "    \n",
    "    # Cr√©er la base vectorielle FAISS\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    print(f\"‚úÖ Base de connaissances cr√©√©e avec {len(texts)} documents\")\n",
    "    return vectorstore\n",
    "\n",
    "# Cr√©er la base de connaissances\n",
    "vectorstore = create_knowledge_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Configuration de la M√©moire Conversationnelle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation_chain(llm, vectorstore):\n",
    "    \"\"\"\n",
    "    Cr√©e une cha√Æne conversationnelle avec m√©moire et RAG\n",
    "    \n",
    "    Args:\n",
    "        llm: Mod√®le de langage\n",
    "        vectorstore: Base de donn√©es vectorielle\n",
    "    \n",
    "    Returns:\n",
    "        ConversationalRetrievalChain: Cha√Æne conversationnelle\n",
    "    \"\"\"\n",
    "    print(\"üîó Configuration de la cha√Æne conversationnelle...\")\n",
    "    \n",
    "    # M√©moire conversationnelle\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"\n",
    "    )\n",
    "    \n",
    "    # Cr√©er la cha√Æne conversationnelle\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 3}  # R√©cup√©rer les 3 documents les plus pertinents\n",
    "        ),\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Cha√Æne conversationnelle configur√©e\")\n",
    "    return qa_chain\n",
    "\n",
    "# Cr√©er la cha√Æne conversationnelle\n",
    "conversation_chain = create_conversation_chain(llm, vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ **Classe Chatbot Principal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbnbLangChainChatbot:\n",
    "    \"\"\"\n",
    "    Chatbot Airbnb utilisant LangChain et HuggingFace\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, conversation_chain):\n",
    "        self.conversation_chain = conversation_chain\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def chat(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Traite une question utilisateur et retourne une r√©ponse\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): Question de l'utilisateur\n",
    "        \n",
    "        Returns:\n",
    "            Dict: R√©ponse avec m√©tadonn√©es\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Obtenir la r√©ponse de la cha√Æne conversationnelle\n",
    "            result = self.conversation_chain({\n",
    "                \"question\": user_input\n",
    "            })\n",
    "            \n",
    "            # Extraire la r√©ponse\n",
    "            answer = result.get(\"answer\", \"D√©sol√©, je n'ai pas pu traiter votre demande.\")\n",
    "            source_docs = result.get(\"source_documents\", [])\n",
    "            \n",
    "            # Ajouter √† l'historique\n",
    "            self.conversation_history.append({\n",
    "                \"user\": user_input,\n",
    "                \"bot\": answer,\n",
    "                \"sources\": len(source_docs)\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": source_docs,\n",
    "                \"confidence\": self._calculate_confidence(answer, source_docs)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Erreur lors du traitement : {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            \n",
    "            return {\n",
    "                \"answer\": \"D√©sol√©, j'ai rencontr√© un probl√®me technique. Pouvez-vous reformuler votre question ?\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "    \n",
    "    def _calculate_confidence(self, answer: str, sources: List) -> float:\n",
    "        \"\"\"\n",
    "        Calcule un score de confiance bas√© sur la r√©ponse et les sources\n",
    "        \"\"\"\n",
    "        if not answer or \"d√©sol√©\" in answer.lower():\n",
    "            return 0.2\n",
    "        \n",
    "        # Plus il y a de sources, plus la confiance est √©lev√©e\n",
    "        source_score = min(len(sources) * 0.3, 0.9)\n",
    "        \n",
    "        # Longueur de la r√©ponse (r√©ponses plus d√©taill√©es = plus de confiance)\n",
    "        length_score = min(len(answer) / 200, 0.5)\n",
    "        \n",
    "        return min(source_score + length_score + 0.3, 1.0)\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Retourne l'historique de conversation\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Efface l'historique de conversation\"\"\"\n",
    "        self.conversation_history = []\n",
    "        # R√©initialiser la m√©moire de la cha√Æne\n",
    "        self.conversation_chain.memory.clear()\n",
    "\n",
    "# Initialiser le chatbot\n",
    "chatbot = AirbnbLangChainChatbot(conversation_chain)\n",
    "print(\"ü§ñ Chatbot LangChain initialis√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ **Tests du Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chatbot():\n",
    "    \"\"\"\n",
    "    Teste le chatbot avec diff√©rentes questions\n",
    "    \"\"\"\n",
    "    print(\"üß™ Tests du chatbot LangChain...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_questions = [\n",
    "        \"Bonjour ! Peux-tu me recommander un h√©bergement √† Hammamet ?\",\n",
    "        \"Quels sont les avantages de Jerba pour des vacances ?\",\n",
    "        \"Je voyage avec ma famille, que me conseilles-tu ?\",\n",
    "        \"Quelle est la meilleure p√©riode pour visiter la Tunisie ?\",\n",
    "        \"Merci pour tes conseils !\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nüôã Question {i}: {question}\")\n",
    "        \n",
    "        response = chatbot.chat(question)\n",
    "        \n",
    "        print(f\"ü§ñ R√©ponse: {response['answer']}\")\n",
    "        print(f\"üìä Confiance: {response['confidence']:.2f}\")\n",
    "        print(f\"üìö Sources utilis√©es: {len(response['sources'])}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Afficher l'historique\n",
    "    print(\"\\nüìú Historique de conversation:\")\n",
    "    for i, conv in enumerate(chatbot.get_conversation_history(), 1):\n",
    "        print(f\"{i}. User: {conv['user'][:50]}...\")\n",
    "        print(f\"   Bot: {conv['bot'][:50]}...\")\n",
    "\n",
    "# Lancer les tests\n",
    "test_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® **Interface Gradio Interactive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"\n",
    "    Cr√©e une interface Gradio pour le chatbot\n",
    "    \"\"\"\n",
    "    def chat_interface(message, history):\n",
    "        \"\"\"\n",
    "        Interface de chat pour Gradio\n",
    "        \"\"\"\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Obtenir la r√©ponse du chatbot\n",
    "        response = chatbot.chat(message)\n",
    "        \n",
    "        # Ajouter √† l'historique Gradio\n",
    "        history.append([message, response['answer']])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    def clear_chat():\n",
    "        \"\"\"\n",
    "        Efface l'historique de chat\n",
    "        \"\"\"\n",
    "        chatbot.clear_history()\n",
    "        return [], \"\"\n",
    "    \n",
    "    # Cr√©er l'interface Gradio\n",
    "    with gr.Blocks(\n",
    "        title=\"üè† Chatbot Airbnb Tunisie - LangChain + HuggingFace\",\n",
    "        theme=gr.themes.Soft()\n",
    "    ) as interface:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # ü§ñ Assistant Airbnb Intelligent\n",
    "        \n",
    "        Powered by **LangChain** + **HuggingFace** üöÄ\n",
    "        \n",
    "        Posez-moi vos questions sur les h√©bergements Airbnb en Tunisie !\n",
    "        \"\"\")\n",
    "        \n",
    "        chatbot_interface = gr.Chatbot(\n",
    "            label=\"üí¨ Conversation\",\n",
    "            height=400,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Votre message\",\n",
    "                placeholder=\"Tapez votre question ici...\",\n",
    "                scale=4\n",
    "            )\n",
    "            send_btn = gr.Button(\"Envoyer üì§\", scale=1)\n",
    "        \n",
    "        with gr.Row():\n",
    "            clear_btn = gr.Button(\"Effacer üóëÔ∏è\")\n",
    "            \n",
    "        # Exemples de questions\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                \"Recommande-moi un h√©bergement √† Hammamet\",\n",
    "                \"Quels sont les avantages de Jerba ?\",\n",
    "                \"Je voyage en famille, que conseilles-tu ?\",\n",
    "                \"Quelle est la meilleure p√©riode pour visiter ?\"\n",
    "            ],\n",
    "            inputs=msg\n",
    "        )\n",
    "        \n",
    "        # √âv√©nements\n",
    "        send_btn.click(\n",
    "            chat_interface,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[chatbot_interface, msg]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            chat_interface,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[chatbot_interface, msg]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_chat,\n",
    "            outputs=[chatbot_interface, msg]\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Cr√©er l'interface\n",
    "print(\"üé® Cr√©ation de l'interface Gradio...\")\n",
    "interface = create_gradio_interface()\n",
    "\n",
    "# Lancer l'interface (d√©commentez pour utiliser)\n",
    "# interface.launch(share=True, debug=True)\n",
    "\n",
    "print(\"‚úÖ Interface Gradio pr√™te ! D√©commentez la ligne ci-dessus pour la lancer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **M√©triques et √âvaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chatbot_performance():\n",
    "    \"\"\"\n",
    "    √âvalue les performances du chatbot\n",
    "    \"\"\"\n",
    "    print(\"üìä √âvaluation des performances du chatbot...\")\n",
    "    \n",
    "    # Questions de test avec r√©ponses attendues\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"Que peux-tu me dire sur Hammamet ?\",\n",
    "            \"expected_keywords\": [\"plage\", \"m√©dina\", \"baln√©aire\", \"tunisie\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Recommandations pour famille ?\",\n",
    "            \"expected_keywords\": [\"famille\", \"piscine\", \"cuisine\", \"villa\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Meilleure p√©riode pour visiter ?\",\n",
    "            \"expected_keywords\": [\"avril\", \"juin\", \"septembre\", \"novembre\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        question = test_case[\"question\"]\n",
    "        expected_keywords = test_case[\"expected_keywords\"]\n",
    "        \n",
    "        # Obtenir la r√©ponse\n",
    "        response = chatbot.chat(question)\n",
    "        answer = response[\"answer\"].lower()\n",
    "        \n",
    "        # Calculer le score de pertinence\n",
    "        found_keywords = sum(1 for keyword in expected_keywords if keyword in answer)\n",
    "        relevance_score = found_keywords / len(expected_keywords)\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"confidence\": response[\"confidence\"],\n",
    "            \"sources_used\": len(response[\"sources\"])\n",
    "        })\n",
    "        \n",
    "        print(f\"‚ùì {question}\")\n",
    "        print(f\"   üìà Pertinence: {relevance_score:.2f}\")\n",
    "        print(f\"   üéØ Confiance: {response['confidence']:.2f}\")\n",
    "        print(f\"   üìö Sources: {len(response['sources'])}\")\n",
    "        print()\n",
    "    \n",
    "    # Statistiques globales\n",
    "    avg_relevance = np.mean([r[\"relevance_score\"] for r in results])\n",
    "    avg_confidence = np.mean([r[\"confidence\"] for r in results])\n",
    "    \n",
    "    print(f\"üìä R√âSULTATS GLOBAUX:\")\n",
    "    print(f\"   üéØ Pertinence moyenne: {avg_relevance:.2f}\")\n",
    "    print(f\"   üí™ Confiance moyenne: {avg_confidence:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# √âvaluer les performances\n",
    "performance_results = evaluate_chatbot_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **R√©sum√© et Prochaines √âtapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary():\n",
    "    \"\"\"\n",
    "    Affiche un r√©sum√© du notebook et les prochaines √©tapes\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "üéâ CHATBOT LANGCHAIN + HUGGINGFACE CR√â√â AVEC SUCC√àS !\n",
    "=====================================================\n",
    "\n",
    "‚úÖ FONCTIONNALIT√âS IMPL√âMENT√âES :\n",
    "   ü§ñ Mod√®le HuggingFace int√©gr√© via LangChain\n",
    "   üß† M√©moire conversationnelle persistante\n",
    "   üîç RAG avec base de connaissances vectorielle\n",
    "   üìö Embeddings multilingues pour la recherche s√©mantique\n",
    "   üé® Interface Gradio interactive\n",
    "   üìä Syst√®me d'√©valuation des performances\n",
    "\n",
    "üõ†Ô∏è TECHNOLOGIES UTILIS√âES :\n",
    "   ‚Ä¢ LangChain : Framework pour applications LLM\n",
    "   ‚Ä¢ HuggingFace Transformers : Mod√®les de langage\n",
    "   ‚Ä¢ FAISS : Base de donn√©es vectorielle\n",
    "   ‚Ä¢ Sentence Transformers : Embeddings s√©mantiques\n",
    "   ‚Ä¢ Gradio : Interface utilisateur interactive\n",
    "\n",
    "üöÄ PROCHAINES √âTAPES RECOMMAND√âES :\n",
    "   1. Tester avec diff√©rents mod√®les HuggingFace\n",
    "   2. Enrichir la base de connaissances avec plus de donn√©es\n",
    "   3. Impl√©menter des m√©triques d'√©valuation avanc√©es\n",
    "   4. D√©ployer sur Hugging Face Spaces ou Streamlit Cloud\n",
    "   5. Ajouter support pour d'autres langues\n",
    "   6. Int√©grer avec une API de donn√©es Airbnb r√©elles\n",
    "\n",
    "üéØ VOTRE CHATBOT EST PR√äT POUR LA PRODUCTION !\n",
    "    \"\"\")\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}